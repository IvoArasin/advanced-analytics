{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Spark and import necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/01 22:15:05 WARN Utils: Your hostname, thinkpad-t470 resolves to a loopback address: 127.0.1.1; using 192.168.0.161 instead (on interface wlp4s0)\n",
      "23/05/01 22:15:05 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/05/01 22:15:06 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# create a local SparkContext object named \"MyApp\" that you can use to interact with Spark // for working with RDDs\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext(\"local\", \"MyApp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a SparkSession object named spark with an app name of \"MyApp\". You can then use it to interact with Spark. // for working with dataframes\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MyApp\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.types import * \n",
    "from pyspark.sql import functions as F\n",
    "from pyspark import StorageLevel\n",
    "from pyspark.ml.feature import *\n",
    "from pyspark.ml.classification import *\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml import PipelineModel\n",
    "from functools import reduce\n",
    "import pandas \n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing data set using the provided stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/linas/anaconda3/envs/pyspark/lib/python3.11/site-packages/pyspark/streaming/context.py:72: FutureWarning: DStream is deprecated as of Spark 3.4.0. Migrate to Structured Streaming.\n",
      "  warnings.warn(\n",
      "23/05/01 22:15:09 WARN StreamingContext: spark.master should be set as local[n], n > 1 in local mode if you have receivers to get data, otherwise Spark jobs will not get resources to process the received data.\n"
     ]
    }
   ],
   "source": [
    "# Helper thread to avoid the Spark StreamingContext from blocking Jupyter\n",
    "        \n",
    "class StreamingThread(threading.Thread):\n",
    "    def __init__(self, ssc):\n",
    "        super().__init__()\n",
    "        self.ssc = ssc\n",
    "    def run(self):\n",
    "        self.ssc.start()\n",
    "        self.ssc.awaitTermination()\n",
    "    def stop(self):\n",
    "        print('----- Stopping... this may take a few seconds -----')\n",
    "        self.ssc.stop(stopSparkContext=False, stopGraceFully=True)\n",
    "        \n",
    "\n",
    "# Start streaming and saving\n",
    "ssc = StreamingContext(sc, 10)\n",
    "lines = ssc.socketTextStream(\"seppe.net\", 7778)\n",
    "lines.saveAsTextFiles(\"/home/linas/Desktop/kul/$em 2/advanced analytics in business/assignment 3/streaming_test/\")\n",
    "ssc_t = StreamingThread(ssc)\n",
    "ssc_t.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Stopping... this may take a few seconds -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/01 22:15:11 WARN ReceiverSupervisorImpl: Skip stopping receiver because it has not yet stared\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Stop streaming\n",
    "ssc_t.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+-----------+-----+\n",
      "|review_id|app_id|review_text|label|\n",
      "+---------+------+-----------+-----+\n",
      "+---------+------+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define empty RDD\n",
    "empty_rdd = spark.sparkContext.emptyRDD()\n",
    "\n",
    "# Define schema for the dataframe\n",
    "columns = StructType([StructField('review_id', StringType(), False),\n",
    "                        StructField('app_id', StringType(), False), \n",
    "                        StructField('review_text', StringType(), False), \n",
    "                        StructField('label', StringType(), False)])\n",
    "\n",
    "# Create empty dataframe\n",
    "df = spark.createDataFrame(data=empty_rdd, schema=columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Populate empty dataframe by reading the files saved after streaming\n",
    "def scan_folder(parent, schema):\n",
    "    for file_name in os.listdir(parent):\n",
    "        if file_name.startswith(\"-168\"):\n",
    "            for text_name in os.listdir(parent+\"/\"+file_name):\n",
    "                if text_name.startswith(\"part\"):\n",
    "                    sub_df=spark.read.format(\"json\").schema(schema).load(parent+\"/\"+file_name+\"/\"+text_name)\n",
    "                    globals()[\"df\"]=globals()[\"df\"].union(sub_df)\n",
    "\n",
    "scan_folder(\"/home/linas/Desktop/kul/$em 2/advanced analytics in business/assignment 3/streaming_test/\", columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+--------------------+-----+\n",
      "|review_id| app_id|         review_text|label|\n",
      "+---------+-------+--------------------+-----+\n",
      "|137107618|2302820|The game is cool ...|    1|\n",
      "|137109815|1663220|Thank you Toge fo...|    1|\n",
      "|137109044|1928870|My son is a minec...|    1|\n",
      "|137110988|1764870|[quote] Love RPG?...|    1|\n",
      "|137109572|1457080|Love the way they...|    1|\n",
      "|137108291|1681060|Good for about 10...|    1|\n",
      "|137108423|1457080|I don't recommend...|    0|\n",
      "|137109720|1862690|v fn,lj/m,naBWhsj...|    1|\n",
      "|136902437|2337890|     Fuскіng awesome|    1|\n",
      "|137108867|2282480|spoiler alert: no...|    0|\n",
      "|137107440|1537830|                  ok|    1|\n",
      "|137110497|1663220|Same setting/game...|    1|\n",
      "|137110332|1743650|DEVELOPER FEAR NO...|    1|\n",
      "|137110615|1457080|This game is fine...|    0|\n",
      "|137110548|1457080|Oyun bazı hikayes...|    1|\n",
      "|137109712|1967630|Personally I did ...|    0|\n",
      "|137109649|1608640|You can play fetc...|    1|\n",
      "+---------+-------+--------------------+-----+\n",
      "\n",
      "(17, 4)\n"
     ]
    }
   ],
   "source": [
    "# Check if the dataframe is populated correctly\n",
    "df.show()\n",
    "print((df.count(), len(df.columns)))\n",
    "# NOTE: shape() is not valid for pyspark dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set label column to integer type\n",
    "df = df.withColumn(\"label\", df[\"label\"].cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing predictive model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and test set\n",
    "df, test= df.randomSplit(weights = [0.70, 0.30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess review data: tokenize\n",
    "step1 = RegexTokenizer(inputCol=\"review_text\", outputCol=\"tokens\", pattern=\"\\\\W\")\n",
    "\n",
    "# Preprocess review data: remove stopwords and add slang words generated by ChatGPT\n",
    "swr = StopWordsRemover(inputCol='tokens', outputCol='filtered_words')\n",
    "w_slang = swr.getStopWords() + ['af', 'aight', 'amirite', 'anyways', 'awesomesauce', 'bae', 'bruh', 'btw', 'dope', 'fam', 'fr', 'gg', 'gj', 'glhf', 'grats', 'imho', 'imo', 'lmao', 'lmfao', 'lol', 'nvm', 'omg', 'pls', 'rip', 'salty', 'savage', 'smh', 'tbh', 'thx', 'ttyl', 'ty', 'wp', 'wtf', 'yeet', 'yolo']\n",
    "step2 = StopWordsRemover(inputCol='tokens', outputCol='filtered_words', stopWords=w_slang)\n",
    "\n",
    "# Preprocess review data: vectorize\n",
    "step3 = CountVectorizer(inputCol=\"filtered_words\", outputCol=\"features\")\n",
    "\n",
    "# Initiate logistic regression model\n",
    "step4 = LogisticRegression(featuresCol='features', labelCol='label')\n",
    "\n",
    "# Create a pipeline\n",
    "pipeline = Pipeline(stages=[step1, step2, step3, step4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Fit pipeline to training data\n",
    "pipelineFit = pipeline.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate model on test data\n",
    "predictions = pipelineFit.transform(test)\n",
    "accuracy = predictions.filter(predictions.label == predictions.prediction).count() / float(test.count())\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Save trained model\n",
    "pipelineFit.save(\"/home/linas/Desktop/kul/$em 2/advanced analytics in business/assignment 3/model/\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using trained model to make predictions as the stream comes in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/01 22:25:05 WARN StreamingContext: spark.master should be set as local[n], n > 1 in local mode if you have receivers to get data, otherwise Spark jobs will not get resources to process the received data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/01 22:25:06 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/05/01 22:25:06 WARN BlockManager: Block input-0-1682969106200 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/05/01 22:25:08 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/05/01 22:25:08 WARN BlockManager: Block input-0-1682969108200 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/05/01 22:25:10 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/05/01 22:25:10 WARN BlockManager: Block input-0-1682969110200 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/05/01 22:25:14 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/05/01 22:25:14 WARN BlockManager: Block input-0-1682969114200 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/05/01 22:25:17 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/05/01 22:25:17 WARN BlockManager: Block input-0-1682969117200 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/05/01 22:25:21 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/05/01 22:25:21 WARN BlockManager: Block input-0-1682969121200 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/05/01 22:25:26 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/05/01 22:25:26 WARN BlockManager: Block input-0-1682969126200 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/05/01 22:25:29 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/05/01 22:25:29 WARN BlockManager: Block input-0-1682969129200 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/05/01 22:25:32 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/05/01 22:25:32 WARN BlockManager: Block input-0-1682969132200 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/05/01 22:25:35 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/05/01 22:25:35 WARN BlockManager: Block input-0-1682969135200 replicated to only 0 peer(s) instead of 1 peers\n"
     ]
    }
   ],
   "source": [
    "# Helper thread to avoid the Spark StreamingContext from blocking Jupyter\n",
    "        \n",
    "class StreamingThread(threading.Thread):\n",
    "    def __init__(self, ssc):\n",
    "        super().__init__()\n",
    "        self.ssc = ssc\n",
    "    def run(self):\n",
    "        self.ssc.start()\n",
    "        self.ssc.awaitTermination()\n",
    "    def stop(self):\n",
    "        print('----- Stopping... this may take a few seconds -----')\n",
    "        self.ssc.stop(stopSparkContext=False, stopGraceFully=True)\n",
    "        \n",
    "\n",
    "# Add capability to predict as the stream comes in\n",
    "globals()['models_loaded'] = False\n",
    "globals()['my_model'] = None\n",
    "\n",
    "def process(time, rdd):\n",
    "    if rdd.isEmpty():\n",
    "        return\n",
    "    \n",
    "    print(\"========= %s =========\" % str(time))\n",
    "    \n",
    "    # Convert to data frame\n",
    "    df = spark.read.json(rdd)\n",
    "    df.show()\n",
    "    # Load in the model if not yet loaded:\n",
    "    if not globals()['models_loaded']:\n",
    "        globals()['my_model'] = PipelineModel.load('/home/linas/Desktop/kul/$em 2/advanced analytics in business/assignment 3/model/')\n",
    "        globals()['models_loaded'] = True\n",
    "    # Make predictions with a loaded model\n",
    "    df_result = globals()['my_model'].transform(df)\n",
    "    df_result.select(\"app_id\", \"label\", \"review_id\", \"review_text\", \"prediction\").show()\n",
    "\n",
    "# Start streaming and predicting\n",
    "ssc = StreamingContext(sc, 10)\n",
    "lines = ssc.socketTextStream(\"seppe.net\", 7778)\n",
    "lines.foreachRDD(process)\n",
    "ssc_t = StreamingThread(ssc)\n",
    "ssc_t.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Stopping... this may take a few seconds -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/01 22:26:00 WARN SocketReceiver: Error receiving data\n",
      "java.net.SocketException: Socket closed\n",
      "\tat java.net.SocketInputStream.socketRead0(Native Method)\n",
      "\tat java.net.SocketInputStream.socketRead(SocketInputStream.java:116)\n",
      "\tat java.net.SocketInputStream.read(SocketInputStream.java:171)\n",
      "\tat java.net.SocketInputStream.read(SocketInputStream.java:141)\n",
      "\tat sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:284)\n",
      "\tat sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:326)\n",
      "\tat sun.nio.cs.StreamDecoder.read(StreamDecoder.java:178)\n",
      "\tat java.io.InputStreamReader.read(InputStreamReader.java:184)\n",
      "\tat java.io.BufferedReader.fill(BufferedReader.java:161)\n",
      "\tat java.io.BufferedReader.readLine(BufferedReader.java:324)\n",
      "\tat java.io.BufferedReader.readLine(BufferedReader.java:389)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.getNext(SocketInputDStream.scala:121)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.getNext(SocketInputDStream.scala:119)\n",
      "\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.receive(SocketInputDStream.scala:91)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$1.run(SocketInputDStream.scala:72)\n",
      "23/05/01 22:26:00 ERROR ReceiverTracker: Deregistered receiver for stream 0: Stopped by driver\n",
      "23/05/01 22:26:00 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Error receiving data\n",
      "java.net.SocketException: Socket closed\n",
      "\tat java.net.SocketInputStream.socketRead0(Native Method)\n",
      "\tat java.net.SocketInputStream.socketRead(SocketInputStream.java:116)\n",
      "\tat java.net.SocketInputStream.read(SocketInputStream.java:171)\n",
      "\tat java.net.SocketInputStream.read(SocketInputStream.java:141)\n",
      "\tat sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:284)\n",
      "\tat sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:326)\n",
      "\tat sun.nio.cs.StreamDecoder.read(StreamDecoder.java:178)\n",
      "\tat java.io.InputStreamReader.read(InputStreamReader.java:184)\n",
      "\tat java.io.BufferedReader.fill(BufferedReader.java:161)\n",
      "\tat java.io.BufferedReader.readLine(BufferedReader.java:324)\n",
      "\tat java.io.BufferedReader.readLine(BufferedReader.java:389)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.getNext(SocketInputDStream.scala:121)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.getNext(SocketInputDStream.scala:119)\n",
      "\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.receive(SocketInputDStream.scala:91)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$1.run(SocketInputDStream.scala:72)\n",
      "23/05/01 22:26:00 WARN ReceiverSupervisorImpl: Receiver has been stopped\n",
      "Exception in thread \"receiver-supervisor-future-0\" java.lang.Error: java.lang.InterruptedException: sleep interrupted\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1155)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: java.lang.InterruptedException: sleep interrupted\n",
      "\tat java.lang.Thread.sleep(Native Method)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:196)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\t... 2 more\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========= 2023-05-01 22:25:10 =========\n",
      "+-------+-----+---------+--------------------+\n",
      "| app_id|label|review_id|         review_text|\n",
      "+-------+-----+---------+--------------------+\n",
      "|1324130|    1|137627587|Much more shallow...|\n",
      "|1324130|    1|137627505|                   .|\n",
      "+-------+-----+---------+--------------------+\n",
      "\n",
      "+-------+-----+---------+--------------------+----------+\n",
      "| app_id|label|review_id|         review_text|prediction|\n",
      "+-------+-----+---------+--------------------+----------+\n",
      "|1324130|    1|137627587|Much more shallow...|       1.0|\n",
      "|1324130|    1|137627505|                   .|       1.0|\n",
      "+-------+-----+---------+--------------------+----------+\n",
      "\n",
      "========= 2023-05-01 22:25:20 =========\n",
      "+-------+-----+---------+--------------------+\n",
      "| app_id|label|review_id|         review_text|\n",
      "+-------+-----+---------+--------------------+\n",
      "|2166060|    1|137630077|a very great game...|\n",
      "|2166060|    1|137629952|Awesome horror in...|\n",
      "|2166060|    1|137628445|bro, i beat the g...|\n",
      "+-------+-----+---------+--------------------+\n",
      "\n",
      "+-------+-----+---------+--------------------+----------+\n",
      "| app_id|label|review_id|         review_text|prediction|\n",
      "+-------+-----+---------+--------------------+----------+\n",
      "|2166060|    1|137630077|a very great game...|       1.0|\n",
      "|2166060|    1|137629952|Awesome horror in...|       1.0|\n",
      "|2166060|    1|137628445|bro, i beat the g...|       1.0|\n",
      "+-------+-----+---------+--------------------+----------+\n",
      "\n",
      "========= 2023-05-01 22:25:30 =========\n",
      "+-------+-----+---------+--------------------+\n",
      "| app_id|label|review_id|         review_text|\n",
      "+-------+-----+---------+--------------------+\n",
      "|2166060|    1|137628163|i love this game,...|\n",
      "|2166060|    1|137627941|This game is real...|\n",
      "|2166060|    1|137627108|Just a normal gam...|\n",
      "+-------+-----+---------+--------------------+\n",
      "\n",
      "+-------+-----+---------+--------------------+----------+\n",
      "| app_id|label|review_id|         review_text|prediction|\n",
      "+-------+-----+---------+--------------------+----------+\n",
      "|2166060|    1|137628163|i love this game,...|       1.0|\n",
      "|2166060|    1|137627941|This game is real...|       0.0|\n",
      "|2166060|    1|137627108|Just a normal gam...|       0.0|\n",
      "+-------+-----+---------+--------------------+----------+\n",
      "\n",
      "========= 2023-05-01 22:25:40 =========\n",
      "+-------+-----+---------+--------------------+\n",
      "| app_id|label|review_id|         review_text|\n",
      "+-------+-----+---------+--------------------+\n",
      "|2166060|    1|137626636|Super fun game! I...|\n",
      "|2166060|    1|137626477|Very interesting ...|\n",
      "+-------+-----+---------+--------------------+\n",
      "\n",
      "+-------+-----+---------+--------------------+----------+\n",
      "| app_id|label|review_id|         review_text|prediction|\n",
      "+-------+-----+---------+--------------------+----------+\n",
      "|2166060|    1|137626636|Super fun game! I...|       0.0|\n",
      "|2166060|    1|137626477|Very interesting ...|       1.0|\n",
      "+-------+-----+---------+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ssc_t.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
