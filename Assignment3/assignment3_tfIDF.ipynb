{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Spark and import necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/19 00:58:18 WARN Utils: Your hostname, thinkpad-t470 resolves to a loopback address: 127.0.1.1; using 192.168.0.164 instead (on interface wlp4s0)\n",
      "23/05/19 00:58:18 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/05/19 00:58:20 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Create a local SparkContext object named \"MyApp\" that you can use to interact with Spark // for rdds\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext(\"local\", \"MyApp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a SparkSession object named spark with an app name of \"MyApp\". You can then use it to interact with Spark. // for dataframes\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MyApp\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.types import * \n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark import StorageLevel\n",
    "from pyspark.ml.feature import *\n",
    "from pyspark.ml.classification import *\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml import PipelineModel\n",
    "from functools import reduce\n",
    "import pandas \n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing data set using the provided stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper thread to avoid the Spark StreamingContext from blocking Jupyter\n",
    "        \n",
    "class StreamingThread(threading.Thread):\n",
    "    def __init__(self, ssc):\n",
    "        super().__init__()\n",
    "        self.ssc = ssc\n",
    "    def run(self):\n",
    "        self.ssc.start()\n",
    "        self.ssc.awaitTermination()\n",
    "    def stop(self):\n",
    "        print('----- Stopping... this may take a few seconds -----')\n",
    "        self.ssc.stop(stopSparkContext=False, stopGraceFully=True)\n",
    "        \n",
    "\n",
    "# Start streaming and saving\n",
    "ssc = StreamingContext(sc, 10)\n",
    "lines = ssc.socketTextStream(\"seppe.net\", 7778)\n",
    "lines.saveAsTextFiles(\"/home/linas/Desktop/kul/$em 2/advanced analytics in business/assignment 3/streaming_test/\")\n",
    "ssc_t = StreamingThread(ssc)\n",
    "ssc_t.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop streaming\n",
    "ssc_t.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+-----------+-----+\n",
      "|review_id|app_id|review_text|label|\n",
      "+---------+------+-----------+-----+\n",
      "+---------+------+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define empty RDD\n",
    "empty_rdd = spark.sparkContext.emptyRDD()\n",
    "\n",
    "# Define schema for the dataframe\n",
    "columns = StructType([StructField('review_id', StringType(), False),\n",
    "                        StructField('app_id', StringType(), False), \n",
    "                        StructField('review_text', StringType(), False), \n",
    "                        StructField('label', StringType(), False)])\n",
    "\n",
    "# Create empty dataframe\n",
    "df = spark.createDataFrame(data=empty_rdd, schema=columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Populate empty dataframe by reading the files saved after streaming\n",
    "def scan_folder(parent, schema):\n",
    "    for file_name in os.listdir(parent):\n",
    "        if file_name.startswith(\"-168\"):\n",
    "            for text_name in os.listdir(parent+\"/\"+file_name):\n",
    "                if text_name.startswith(\"part\"):\n",
    "                    sub_df=spark.read.format(\"json\").schema(schema).load(parent+\"/\"+file_name+\"/\"+text_name)\n",
    "                    globals()[\"df\"]=globals()[\"df\"].union(sub_df)\n",
    "\n",
    "scan_folder(\"/home/linas/Desktop/kul/$em 2/advanced analytics in business/assignment 3/data/streaming_data_subsample/\", columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicate rows\n",
    "df = df.dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove empty reviews\n",
    "df = df.filter(df.review_text != \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set label column to integer type\n",
    "df = df.withColumn(\"label\", df[\"label\"].cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/18 22:15:34 WARN DAGScheduler: Broadcasting large task binary with size 6.0 MiB\n",
      "23/05/18 22:18:50 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------------+--------------------+-----+\n",
      "|review_id|         app_id|         review_text|label|\n",
      "+---------+---------------+--------------------+-----+\n",
      "|136616773|         955200|       Ebin :DDDDDDD|    1|\n",
      "|136616125|        2328760|Downgrade compare...|    0|\n",
      "|136616752|        2328760|Step 1: Abandon y...|    0|\n",
      "|136619845|        1913870|Incredible party ...|    1|\n",
      "|136633177|        1498040|If you like theor...|    1|\n",
      "|136635517|        1811990|Adventure Time an...|    1|\n",
      "|136633228|        1772830|Played a couple h...|    1|\n",
      "|136635511|        1811990|first steam revie...|    0|\n",
      "|136755504|         307950|Played 51 minutes...|    0|\n",
      "|136758662|        1527950|WhiteTales... I m...|    0|\n",
      "|136755179|         307950|          cool game.|    1|\n",
      "|136757637|        1527950|I'm addicted. The...|    1|\n",
      "|136667892|1798010,1798020|I want to convinc...|    1|\n",
      "|136667372|1798010,1798020|battle network 1 ...|    1|\n",
      "|136667726|1798010,1798020|My childhood! \\nA...|    1|\n",
      "|136667270|1798010,1798020|Played before on ...|    1|\n",
      "|136667060|1798010,1798020|A classic series ...|    1|\n",
      "|136760666|        1798010|I remember playin...|    1|\n",
      "|136758902|        2273470|A new simulator. ...|    1|\n",
      "|136758985|        2273470|A small and fun d...|    1|\n",
      "+---------+---------------+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/18 22:19:19 WARN DAGScheduler: Broadcasting large task binary with size 6.0 MiB\n",
      "23/05/18 22:22:32 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(793, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Check if the dataframe is populated correctly\n",
    "df.select(df.columns[:20]).show()\n",
    "print((df.count(), len(df.columns)))\n",
    "# NOTE: shape() is not valid for pyspark dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/18 22:22:58 WARN DAGScheduler: Broadcasting large task binary with size 6.0 MiB\n",
      "23/05/18 22:26:18 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "23/05/18 22:26:20 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|label|count|\n",
      "+-----+-----+\n",
      "|    0|  146|\n",
      "|    1|  647|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Count class distribution\n",
    "count_df = df.groupBy('label').count().orderBy('label')\n",
    "count_df.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing predictive model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and test set\n",
    "df, test= df.randomSplit(weights = [0.70, 0.30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Preprocess review data: tokenize\n",
    "# step1 = RegexTokenizer(inputCol=\"review_text\", outputCol=\"tokens\", pattern=\"\\\\W\")\n",
    "\n",
    "# # Preprocess review data: remove stopwords and add slang words generated by ChatGPT\n",
    "# swr = StopWordsRemover(inputCol='tokens', outputCol='filtered_words')\n",
    "# w_slang = swr.getStopWords() + ['af', 'aight', 'amirite', 'anyways', 'awesomesauce', 'bae', 'bruh', 'btw', 'dope', 'fam', 'fr', 'gg', 'gj', 'glhf', 'grats', 'imho', 'imo', 'lmao', 'lmfao', 'lol', 'nvm', 'omg', 'pls', 'rip', 'salty', 'savage', 'smh', 'tbh', 'thx', 'ttyl', 'ty', 'wp', 'wtf', 'yeet', 'yolo']\n",
    "# step2 = StopWordsRemover(inputCol='tokens', outputCol='filtered_words', stopWords=w_slang)\n",
    "\n",
    "# # Preprocess review data: vectorize\n",
    "# step3 = CountVectorizer(inputCol=\"filtered_words\", outputCol=\"features\")\n",
    "\n",
    "# # Initiate logistic regression model\n",
    "# step4 = LogisticRegression(featuresCol='features', labelCol='label')\n",
    "\n",
    "# # Create pipeline\n",
    "# pipeline = Pipeline(stages=[step1, step2, step3, step4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess review data: tokenize\n",
    "step1 = RegexTokenizer(inputCol=\"review_text\", outputCol=\"tokens\", pattern=\"\\\\W\")\n",
    "\n",
    "# Preprocess review data: remove stopwords and add slang words generated by ChatGPT\n",
    "swr = StopWordsRemover(inputCol='tokens', outputCol='filtered_words')\n",
    "w_slang = swr.getStopWords() + ['af', 'aight', 'amirite', 'anyways', 'awesomesauce', 'bae', 'bruh', 'btw', 'dope', 'fam', 'fr', 'gg', 'gj', 'glhf', 'grats', 'imho', 'imo', 'lmao', 'lmfao', 'lol', 'nvm', 'omg', 'pls', 'rip', 'salty', 'savage', 'smh', 'tbh', 'thx', 'ttyl', 'ty', 'wp', 'wtf', 'yeet', 'yolo']\n",
    "step2 = StopWordsRemover(inputCol='tokens', outputCol='filtered_words', stopWords=w_slang)\n",
    "\n",
    "# Preprocess review data: compute term frequency\n",
    "step3 = HashingTF(inputCol=\"filtered_words\", outputCol=\"term_freq\", numFeatures=2**16)\n",
    "\n",
    "# Preprocess review data: compute TF-IDF features\n",
    "step4 = IDF(inputCol=\"term_freq\", outputCol=\"features\")\n",
    "\n",
    "# Initiate logistic regression model\n",
    "step5 = LogisticRegression(featuresCol='features', labelCol='label')\n",
    "\n",
    "# Create pipeline\n",
    "pipeline = Pipeline(stages=[step1, step2, step3, step4, step5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/18 22:26:47 WARN DAGScheduler: Broadcasting large task binary with size 5.6 MiB\n",
      "23/05/18 22:29:47 WARN DAGScheduler: Broadcasting large task binary with size 2.9 MiB\n",
      "23/05/18 22:30:15 WARN DAGScheduler: Broadcasting large task binary with size 5.6 MiB\n",
      "23/05/18 22:33:39 WARN DAGScheduler: Broadcasting large task binary with size 5.6 MiB\n",
      "23/05/18 22:37:26 WARN DAGScheduler: Broadcasting large task binary with size 4.0 MiB\n",
      "23/05/18 22:37:27 WARN DAGScheduler: Broadcasting large task binary with size 4.0 MiB\n",
      "23/05/18 22:37:30 WARN DAGScheduler: Broadcasting large task binary with size 4.0 MiB\n",
      "23/05/18 22:37:31 WARN DAGScheduler: Broadcasting large task binary with size 4.0 MiB\n",
      "23/05/18 22:37:31 WARN DAGScheduler: Broadcasting large task binary with size 4.0 MiB\n",
      "23/05/18 22:37:32 WARN DAGScheduler: Broadcasting large task binary with size 4.0 MiB\n",
      "23/05/18 22:37:32 WARN DAGScheduler: Broadcasting large task binary with size 4.0 MiB\n",
      "23/05/18 22:37:33 WARN DAGScheduler: Broadcasting large task binary with size 4.0 MiB\n",
      "23/05/18 22:37:33 WARN DAGScheduler: Broadcasting large task binary with size 4.0 MiB\n",
      "23/05/18 22:37:34 WARN DAGScheduler: Broadcasting large task binary with size 4.0 MiB\n",
      "23/05/18 22:37:34 WARN DAGScheduler: Broadcasting large task binary with size 4.0 MiB\n",
      "23/05/18 22:37:35 WARN DAGScheduler: Broadcasting large task binary with size 4.0 MiB\n",
      "23/05/18 22:37:35 WARN DAGScheduler: Broadcasting large task binary with size 4.0 MiB\n",
      "23/05/18 22:37:36 WARN DAGScheduler: Broadcasting large task binary with size 4.0 MiB\n",
      "23/05/18 22:37:37 WARN DAGScheduler: Broadcasting large task binary with size 4.0 MiB\n",
      "23/05/18 22:37:37 WARN DAGScheduler: Broadcasting large task binary with size 4.0 MiB\n",
      "23/05/18 22:37:38 WARN DAGScheduler: Broadcasting large task binary with size 4.0 MiB\n",
      "23/05/18 22:37:38 WARN DAGScheduler: Broadcasting large task binary with size 4.0 MiB\n",
      "23/05/18 22:37:39 WARN DAGScheduler: Broadcasting large task binary with size 4.0 MiB\n",
      "23/05/18 22:37:39 WARN DAGScheduler: Broadcasting large task binary with size 4.0 MiB\n",
      "23/05/18 22:37:40 WARN DAGScheduler: Broadcasting large task binary with size 4.0 MiB\n",
      "23/05/18 22:37:40 WARN DAGScheduler: Broadcasting large task binary with size 4.0 MiB\n",
      "23/05/18 22:37:41 WARN DAGScheduler: Broadcasting large task binary with size 4.0 MiB\n",
      "23/05/18 22:37:42 WARN DAGScheduler: Broadcasting large task binary with size 4.0 MiB\n",
      "23/05/18 22:37:42 WARN DAGScheduler: Broadcasting large task binary with size 4.0 MiB\n",
      "23/05/18 22:37:42 WARN DAGScheduler: Broadcasting large task binary with size 4.0 MiB\n",
      "23/05/18 22:37:43 WARN DAGScheduler: Broadcasting large task binary with size 4.0 MiB\n",
      "23/05/18 22:37:43 WARN DAGScheduler: Broadcasting large task binary with size 4.0 MiB\n",
      "23/05/18 22:37:44 WARN DAGScheduler: Broadcasting large task binary with size 4.0 MiB\n",
      "23/05/18 22:37:44 WARN DAGScheduler: Broadcasting large task binary with size 4.0 MiB\n",
      "23/05/18 22:37:45 WARN DAGScheduler: Broadcasting large task binary with size 4.0 MiB\n",
      "23/05/18 22:37:45 WARN DAGScheduler: Broadcasting large task binary with size 4.0 MiB\n",
      "23/05/18 22:37:46 WARN DAGScheduler: Broadcasting large task binary with size 4.0 MiB\n",
      "23/05/18 22:37:46 WARN DAGScheduler: Broadcasting large task binary with size 4.0 MiB\n",
      "23/05/18 22:37:47 WARN DAGScheduler: Broadcasting large task binary with size 4.0 MiB\n",
      "23/05/18 22:37:47 WARN DAGScheduler: Broadcasting large task binary with size 4.0 MiB\n",
      "23/05/18 22:38:45 WARN DAGScheduler: Broadcasting large task binary with size 5.6 MiB\n",
      "23/05/18 22:59:49 WARN DAGScheduler: Broadcasting large task binary with size 5.6 MiB\n",
      "[Stage 93:===================================================>(1075 + 1) / 1076]\r"
     ]
    }
   ],
   "source": [
    "# Fit pipeline to training data\n",
    "pipelineFit = pipeline.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/18 23:29:46 WARN DAGScheduler: Broadcasting large task binary with size 6.0 MiB\n",
      "23/05/18 23:57:20 WARN DAGScheduler: Broadcasting large task binary with size 4.4 MiB\n",
      "23/05/18 23:57:58 WARN DAGScheduler: Broadcasting large task binary with size 6.0 MiB\n",
      "23/05/19 00:39:58 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8130081300813008"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Validate model on test data\n",
    "predictions = pipelineFit.transform(test)\n",
    "accuracy = predictions.filter(predictions.label == predictions.prediction).count() / float(test.count())\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/19 00:40:12 WARN TaskSetManager: Stage 111 contains a task of very large size (1052 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Save trained model\n",
    "pipelineFit.save(\"/home/linas/Desktop/kul/$em 2/advanced analytics in business/assignment 3/model_tf_idf/\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use trained model to make predictions as the stream comes in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper thread to avoid the Spark StreamingContext from blocking Jupyter\n",
    "        \n",
    "class StreamingThread(threading.Thread):\n",
    "    def __init__(self, ssc):\n",
    "        super().__init__()\n",
    "        self.ssc = ssc\n",
    "    def run(self):\n",
    "        self.ssc.start()\n",
    "        self.ssc.awaitTermination()\n",
    "    def stop(self):\n",
    "        print('----- Stopping... this may take a few seconds -----')\n",
    "        self.ssc.stop(stopSparkContext=False, stopGraceFully=True)\n",
    "        \n",
    "\n",
    "# Add capability to predict as the stream comes in\n",
    "globals()['models_loaded'] = False\n",
    "globals()['my_model'] = None\n",
    "\n",
    "def process(time, rdd):\n",
    "    if rdd.isEmpty():\n",
    "        return\n",
    "    \n",
    "    print(\"========= %s =========\" % str(time))\n",
    "    \n",
    "    # Convert to data frame\n",
    "    df = spark.read.json(rdd)\n",
    "    df.show()\n",
    "    # Load in the model if not yet loaded:\n",
    "    if not globals()['models_loaded']:\n",
    "        # load in your models here\n",
    "        globals()['my_model'] = PipelineModel.load('/home/linas/Desktop/kul/$em 2/advanced analytics in business/assignment 3/model_tf_idf/')\n",
    "        globals()['models_loaded'] = True\n",
    "    # Make predictions with loaded model\n",
    "    df_result = globals()['my_model'].transform(df)\n",
    "    df_result.select(\"app_id\", \"label\", \"review_id\", \"review_text\", \"prediction\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/linas/anaconda3/envs/pyspark/lib/python3.11/site-packages/pyspark/streaming/context.py:72: FutureWarning: DStream is deprecated as of Spark 3.4.0. Migrate to Structured Streaming.\n",
      "  warnings.warn(\n",
      "23/05/19 00:58:39 WARN StreamingContext: spark.master should be set as local[n], n > 1 in local mode if you have receivers to get data, otherwise Spark jobs will not get resources to process the received data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/19 00:58:42 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/05/19 00:58:42 WARN BlockManager: Block input-0-1684450722200 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/05/19 00:58:44 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/05/19 00:58:44 WARN BlockManager: Block input-0-1684450724200 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/05/19 00:58:48 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/05/19 00:58:48 WARN BlockManager: Block input-0-1684450728400 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/05/19 00:58:53 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/05/19 00:58:53 WARN BlockManager: Block input-0-1684450733200 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/05/19 00:58:57 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/05/19 00:58:57 WARN BlockManager: Block input-0-1684450737400 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/05/19 00:59:02 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/05/19 00:59:02 WARN BlockManager: Block input-0-1684450742400 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/05/19 00:59:05 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/05/19 00:59:05 WARN BlockManager: Block input-0-1684450745200 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/05/19 00:59:08 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/05/19 00:59:08 WARN BlockManager: Block input-0-1684450748400 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/05/19 00:59:13 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/05/19 00:59:13 WARN BlockManager: Block input-0-1684450753400 replicated to only 0 peer(s) instead of 1 peers\n"
     ]
    }
   ],
   "source": [
    "# Start streaming and predicting\n",
    "ssc = StreamingContext(sc, 10)\n",
    "lines = ssc.socketTextStream(\"seppe.net\", 7778)\n",
    "lines.foreachRDD(process)\n",
    "ssc_t = StreamingThread(ssc)\n",
    "ssc_t.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Stopping... this may take a few seconds -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/19 00:59:23 WARN SocketReceiver: Error receiving data\n",
      "java.net.SocketException: Socket closed\n",
      "\tat java.net.SocketInputStream.socketRead0(Native Method)\n",
      "\tat java.net.SocketInputStream.socketRead(SocketInputStream.java:116)\n",
      "\tat java.net.SocketInputStream.read(SocketInputStream.java:171)\n",
      "\tat java.net.SocketInputStream.read(SocketInputStream.java:141)\n",
      "\tat sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:284)\n",
      "\tat sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:326)\n",
      "\tat sun.nio.cs.StreamDecoder.read(StreamDecoder.java:178)\n",
      "\tat java.io.InputStreamReader.read(InputStreamReader.java:184)\n",
      "\tat java.io.BufferedReader.fill(BufferedReader.java:161)\n",
      "\tat java.io.BufferedReader.readLine(BufferedReader.java:324)\n",
      "\tat java.io.BufferedReader.readLine(BufferedReader.java:389)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.getNext(SocketInputDStream.scala:121)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.getNext(SocketInputDStream.scala:119)\n",
      "\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.receive(SocketInputDStream.scala:91)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$1.run(SocketInputDStream.scala:72)\n",
      "23/05/19 00:59:23 ERROR ReceiverTracker: Deregistered receiver for stream 0: Stopped by driver\n",
      "23/05/19 00:59:23 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Error receiving data\n",
      "java.net.SocketException: Socket closed\n",
      "\tat java.net.SocketInputStream.socketRead0(Native Method)\n",
      "\tat java.net.SocketInputStream.socketRead(SocketInputStream.java:116)\n",
      "\tat java.net.SocketInputStream.read(SocketInputStream.java:171)\n",
      "\tat java.net.SocketInputStream.read(SocketInputStream.java:141)\n",
      "\tat sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:284)\n",
      "\tat sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:326)\n",
      "\tat sun.nio.cs.StreamDecoder.read(StreamDecoder.java:178)\n",
      "\tat java.io.InputStreamReader.read(InputStreamReader.java:184)\n",
      "\tat java.io.BufferedReader.fill(BufferedReader.java:161)\n",
      "\tat java.io.BufferedReader.readLine(BufferedReader.java:324)\n",
      "\tat java.io.BufferedReader.readLine(BufferedReader.java:389)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.getNext(SocketInputDStream.scala:121)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.getNext(SocketInputDStream.scala:119)\n",
      "\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.receive(SocketInputDStream.scala:91)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$1.run(SocketInputDStream.scala:72)\n",
      "23/05/19 00:59:23 WARN ReceiverSupervisorImpl: Receiver has been stopped\n",
      "Exception in thread \"receiver-supervisor-future-0\" java.lang.Error: java.lang.InterruptedException: sleep interrupted\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1155)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: java.lang.InterruptedException: sleep interrupted\n",
      "\tat java.lang.Thread.sleep(Native Method)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:196)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\t... 2 more\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========= 2023-05-19 00:58:50 =========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+---------+--------------------+\n",
      "| app_id|label|review_id|         review_text|\n",
      "+-------+-----+---------+--------------------+\n",
      "|2363140|    1|138507859|yep. don't worry ...|\n",
      "|2311190|    0|138508617|it sucks my scree...|\n",
      "|1742020|    1|138509366|    👏👏 Next Meme~!|\n",
      "+-------+-----+---------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/19 00:59:43 WARN DAGScheduler: Broadcasting large task binary with size 1145.2 KiB\n",
      "23/05/19 00:59:44 WARN DAGScheduler: Broadcasting large task binary with size 1145.2 KiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+---------+--------------------+----------+\n",
      "| app_id|label|review_id|         review_text|prediction|\n",
      "+-------+-----+---------+--------------------+----------+\n",
      "|2363140|    1|138507859|yep. don't worry ...|       1.0|\n",
      "|2311190|    0|138508617|it sucks my scree...|       1.0|\n",
      "|1742020|    1|138509366|    👏👏 Next Meme~!|       1.0|\n",
      "+-------+-----+---------+--------------------+----------+\n",
      "\n",
      "========= 2023-05-19 00:59:00 =========\n",
      "+-------+-----+---------+--------------------+\n",
      "| app_id|label|review_id|         review_text|\n",
      "+-------+-----+---------+--------------------+\n",
      "|1742020|    1|138507426|No one reads the ...|\n",
      "|1742020|    1|138506738|Never in my life ...|\n",
      "+-------+-----+---------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/19 00:59:46 WARN DAGScheduler: Broadcasting large task binary with size 1145.2 KiB\n",
      "23/05/19 00:59:47 WARN DAGScheduler: Broadcasting large task binary with size 1145.2 KiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+---------+--------------------+----------+\n",
      "| app_id|label|review_id|         review_text|prediction|\n",
      "+-------+-----+---------+--------------------+----------+\n",
      "|1742020|    1|138507426|No one reads the ...|       1.0|\n",
      "|1742020|    1|138506738|Never in my life ...|       1.0|\n",
      "+-------+-----+---------+--------------------+----------+\n",
      "\n",
      "========= 2023-05-19 00:59:10 =========\n",
      "+-------+-----+---------+--------------------+\n",
      "| app_id|label|review_id|         review_text|\n",
      "+-------+-----+---------+--------------------+\n",
      "|1940340|    0|138509031|Unfinished game w...|\n",
      "|1940340|    1|138508821|You will suffer. ...|\n",
      "|1940340|    1|138508636|He may look like ...|\n",
      "+-------+-----+---------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/19 00:59:49 WARN DAGScheduler: Broadcasting large task binary with size 1145.2 KiB\n",
      "23/05/19 00:59:49 WARN DAGScheduler: Broadcasting large task binary with size 1145.2 KiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+---------+--------------------+----------+\n",
      "| app_id|label|review_id|         review_text|prediction|\n",
      "+-------+-----+---------+--------------------+----------+\n",
      "|1940340|    0|138509031|Unfinished game w...|       0.0|\n",
      "|1940340|    1|138508821|You will suffer. ...|       1.0|\n",
      "|1940340|    1|138508636|He may look like ...|       1.0|\n",
      "+-------+-----+---------+--------------------+----------+\n",
      "\n",
      "========= 2023-05-19 00:59:20 =========\n",
      "+-------+-----+---------+--------------------+\n",
      "| app_id|label|review_id|         review_text|\n",
      "+-------+-----+---------+--------------------+\n",
      "|1940340|    1|138507971|I like how devs e...|\n",
      "|1940340|    1|138507795|            gud game|\n",
      "+-------+-----+---------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/19 00:59:51 WARN DAGScheduler: Broadcasting large task binary with size 1145.2 KiB\n",
      "23/05/19 00:59:51 WARN DAGScheduler: Broadcasting large task binary with size 1145.2 KiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+---------+--------------------+----------+\n",
      "| app_id|label|review_id|         review_text|prediction|\n",
      "+-------+-----+---------+--------------------+----------+\n",
      "|1940340|    1|138507971|I like how devs e...|       1.0|\n",
      "|1940340|    1|138507795|            gud game|       1.0|\n",
      "+-------+-----+---------+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ssc_t.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
