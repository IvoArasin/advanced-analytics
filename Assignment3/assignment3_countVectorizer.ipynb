{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Spark and import necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/19 13:11:04 WARN Utils: Your hostname, thinkpad-t470 resolves to a loopback address: 127.0.1.1; using 192.168.0.164 instead (on interface wlp4s0)\n",
      "23/05/19 13:11:04 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/05/19 13:11:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Create a local SparkContext object named \"MyApp\" that you can use to interact with Spark // for rdds\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext(\"local\", \"MyApp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a SparkSession object named spark with an app name of \"MyApp\". You can then use it to interact with Spark. // for dataframes\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MyApp\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.types import * \n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark import StorageLevel\n",
    "from pyspark.ml.feature import *\n",
    "from pyspark.ml.classification import *\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml import PipelineModel\n",
    "from functools import reduce\n",
    "import pandas \n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing data set using the provided stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/linas/anaconda3/envs/pyspark/lib/python3.11/site-packages/pyspark/streaming/context.py:72: FutureWarning: DStream is deprecated as of Spark 3.4.0. Migrate to Structured Streaming.\n",
      "  warnings.warn(\n",
      "23/05/08 11:37:55 WARN StreamingContext: spark.master should be set as local[n], n > 1 in local mode if you have receivers to get data, otherwise Spark jobs will not get resources to process the received data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/08 11:37:57 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/05/08 11:37:57 WARN BlockManager: Block input-0-1683535077600 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/05/08 11:38:00 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/05/08 11:38:00 WARN BlockManager: Block input-0-1683535080600 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/05/08 11:38:02 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/05/08 11:38:02 WARN BlockManager: Block input-0-1683535082600 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/05/08 11:38:08 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/05/08 11:38:08 WARN BlockManager: Block input-0-1683535087800 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/05/08 11:38:12 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/05/08 11:38:12 WARN BlockManager: Block input-0-1683535092600 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/05/08 11:38:17 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/05/08 11:38:17 WARN BlockManager: Block input-0-1683535097600 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/05/08 11:38:21 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/05/08 11:38:21 WARN BlockManager: Block input-0-1683535101600 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/05/08 11:38:25 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/05/08 11:38:25 WARN BlockManager: Block input-0-1683535105600 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/05/08 11:38:28 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/05/08 11:38:28 WARN BlockManager: Block input-0-1683535108600 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/05/08 11:38:33 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/05/08 11:38:33 WARN BlockManager: Block input-0-1683535113600 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/05/08 11:38:57 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/05/08 11:38:57 WARN BlockManager: Block input-0-1683535136800 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/05/08 11:39:01 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/05/08 11:39:01 WARN BlockManager: Block input-0-1683535140800 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/05/08 11:39:02 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/05/08 11:39:02 WARN BlockManager: Block input-0-1683535141800 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/05/08 11:39:06 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/05/08 11:39:06 WARN BlockManager: Block input-0-1683535145800 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/05/08 11:39:10 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/05/08 11:39:10 WARN BlockManager: Block input-0-1683535149800 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/05/08 11:39:13 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/05/08 11:39:13 WARN BlockManager: Block input-0-1683535152800 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/05/08 11:39:18 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/05/08 11:39:18 WARN BlockManager: Block input-0-1683535157800 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/05/08 11:39:22 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/05/08 11:39:22 WARN BlockManager: Block input-0-1683535161800 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/05/08 11:39:27 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/05/08 11:39:27 WARN BlockManager: Block input-0-1683535167000 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/05/08 11:39:31 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/05/08 11:39:31 WARN BlockManager: Block input-0-1683535170800 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/05/08 11:39:45 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/05/08 11:39:45 WARN BlockManager: Block input-0-1683535184800 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/05/08 11:39:46 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/05/08 11:39:46 WARN BlockManager: Block input-0-1683535185800 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/05/08 11:39:51 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/05/08 11:39:51 WARN BlockManager: Block input-0-1683535190800 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/05/08 11:39:53 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/05/08 11:39:53 WARN BlockManager: Block input-0-1683535192800 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/05/08 11:39:57 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/05/08 11:39:57 WARN BlockManager: Block input-0-1683535196800 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/05/08 11:40:00 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/05/08 11:40:00 WARN BlockManager: Block input-0-1683535199800 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/05/08 11:40:01 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/05/08 11:40:01 WARN BlockManager: Block input-0-1683535200800 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/05/08 11:40:05 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/05/08 11:40:05 WARN BlockManager: Block input-0-1683535204800 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/05/08 11:40:10 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/05/08 11:40:10 WARN BlockManager: Block input-0-1683535209800 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/05/08 11:40:13 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/05/08 11:40:13 WARN BlockManager: Block input-0-1683535212800 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/05/08 11:40:27 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/05/08 11:40:27 WARN BlockManager: Block input-0-1683535227000 replicated to only 0 peer(s) instead of 1 peers\n",
      "[Stage 0:>                  (0 + 1) / 1][Stage 1:>                  (0 + 0) / 1]\r"
     ]
    }
   ],
   "source": [
    "# Helper thread to avoid the Spark StreamingContext from blocking Jupyter\n",
    "        \n",
    "class StreamingThread(threading.Thread):\n",
    "    def __init__(self, ssc):\n",
    "        super().__init__()\n",
    "        self.ssc = ssc\n",
    "    def run(self):\n",
    "        self.ssc.start()\n",
    "        self.ssc.awaitTermination()\n",
    "    def stop(self):\n",
    "        print('----- Stopping... this may take a few seconds -----')\n",
    "        self.ssc.stop(stopSparkContext=False, stopGraceFully=True)\n",
    "        \n",
    "\n",
    "# Start streaming and saving\n",
    "ssc = StreamingContext(sc, 10)\n",
    "lines = ssc.socketTextStream(\"seppe.net\", 7778)\n",
    "lines.saveAsTextFiles(\"/home/linas/Desktop/kul/$em 2/advanced analytics in business/assignment 3/streaming_test/\")\n",
    "ssc_t = StreamingThread(ssc)\n",
    "ssc_t.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Stopping... this may take a few seconds -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/08 11:42:30 WARN SocketReceiver: Error receiving data\n",
      "java.net.SocketException: Socket closed\n",
      "\tat java.net.SocketInputStream.socketRead0(Native Method)\n",
      "\tat java.net.SocketInputStream.socketRead(SocketInputStream.java:116)\n",
      "\tat java.net.SocketInputStream.read(SocketInputStream.java:171)\n",
      "\tat java.net.SocketInputStream.read(SocketInputStream.java:141)\n",
      "\tat sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:284)\n",
      "\tat sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:326)\n",
      "\tat sun.nio.cs.StreamDecoder.read(StreamDecoder.java:178)\n",
      "\tat java.io.InputStreamReader.read(InputStreamReader.java:184)\n",
      "\tat java.io.BufferedReader.fill(BufferedReader.java:161)\n",
      "\tat java.io.BufferedReader.readLine(BufferedReader.java:324)\n",
      "\tat java.io.BufferedReader.readLine(BufferedReader.java:389)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.getNext(SocketInputDStream.scala:121)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.getNext(SocketInputDStream.scala:119)\n",
      "\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.receive(SocketInputDStream.scala:91)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$1.run(SocketInputDStream.scala:72)\n",
      "23/05/08 11:42:30 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Error receiving data\n",
      "java.net.SocketException: Socket closed\n",
      "\tat java.net.SocketInputStream.socketRead0(Native Method)\n",
      "\tat java.net.SocketInputStream.socketRead(SocketInputStream.java:116)\n",
      "\tat java.net.SocketInputStream.read(SocketInputStream.java:171)\n",
      "\tat java.net.SocketInputStream.read(SocketInputStream.java:141)\n",
      "\tat sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:284)\n",
      "\tat sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:326)\n",
      "\tat sun.nio.cs.StreamDecoder.read(StreamDecoder.java:178)\n",
      "\tat java.io.InputStreamReader.read(InputStreamReader.java:184)\n",
      "\tat java.io.BufferedReader.fill(BufferedReader.java:161)\n",
      "\tat java.io.BufferedReader.readLine(BufferedReader.java:324)\n",
      "\tat java.io.BufferedReader.readLine(BufferedReader.java:389)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.getNext(SocketInputDStream.scala:121)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.getNext(SocketInputDStream.scala:119)\n",
      "\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.receive(SocketInputDStream.scala:91)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$1.run(SocketInputDStream.scala:72)\n",
      "23/05/08 11:42:30 ERROR ReceiverTracker: Deregistered receiver for stream 0: Stopped by driver\n",
      "23/05/08 11:42:30 WARN ReceiverSupervisorImpl: Receiver has been stopped\n",
      "Exception in thread \"receiver-supervisor-future-0\" java.lang.Error: java.lang.InterruptedException: sleep interrupted\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1155)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: java.lang.InterruptedException: sleep interrupted\n",
      "\tat java.lang.Thread.sleep(Native Method)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:196)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\t... 2 more\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Stop streaming\n",
    "ssc_t.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+-----------+-----+\n",
      "|review_id|app_id|review_text|label|\n",
      "+---------+------+-----------+-----+\n",
      "+---------+------+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define empty RDD\n",
    "empty_rdd = spark.sparkContext.emptyRDD()\n",
    "\n",
    "# Define schema for the dataframe\n",
    "columns = StructType([StructField('review_id', StringType(), False),\n",
    "                        StructField('app_id', StringType(), False), \n",
    "                        StructField('review_text', StringType(), False), \n",
    "                        StructField('label', StringType(), False)])\n",
    "\n",
    "# Create empty dataframe\n",
    "df = spark.createDataFrame(data=empty_rdd, schema=columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Populate empty dataframe by reading the files saved after streaming\n",
    "def scan_folder(parent, schema):\n",
    "    for file_name in os.listdir(parent):\n",
    "        if file_name.startswith(\"-168\"):\n",
    "            for text_name in os.listdir(parent+\"/\"+file_name):\n",
    "                if text_name.startswith(\"part\"):\n",
    "                    sub_df=spark.read.format(\"json\").schema(schema).load(parent+\"/\"+file_name+\"/\"+text_name)\n",
    "                    globals()[\"df\"]=globals()[\"df\"].union(sub_df)\n",
    "\n",
    "scan_folder(\"/home/linas/Desktop/kul/$em 2/advanced analytics in business/assignment 3/data/streaming_data_subsample/\", columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicate rows\n",
    "df = df.dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove empty reviews\n",
    "df = df.filter(df.review_text != \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set label column to integer type\n",
    "df = df.withColumn(\"label\", df[\"label\"].cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/18 19:29:27 WARN DAGScheduler: Broadcasting large task binary with size 6.0 MiB\n",
      "23/05/18 19:32:43 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------------+--------------------+-----+\n",
      "|review_id|         app_id|         review_text|label|\n",
      "+---------+---------------+--------------------+-----+\n",
      "|136616773|         955200|       Ebin :DDDDDDD|    1|\n",
      "|136616125|        2328760|Downgrade compare...|    0|\n",
      "|136616752|        2328760|Step 1: Abandon y...|    0|\n",
      "|136619845|        1913870|Incredible party ...|    1|\n",
      "|136633177|        1498040|If you like theor...|    1|\n",
      "|136635517|        1811990|Adventure Time an...|    1|\n",
      "|136633228|        1772830|Played a couple h...|    1|\n",
      "|136635511|        1811990|first steam revie...|    0|\n",
      "|136755504|         307950|Played 51 minutes...|    0|\n",
      "|136758662|        1527950|WhiteTales... I m...|    0|\n",
      "|136755179|         307950|          cool game.|    1|\n",
      "|136757637|        1527950|I'm addicted. The...|    1|\n",
      "|136667892|1798010,1798020|I want to convinc...|    1|\n",
      "|136667372|1798010,1798020|battle network 1 ...|    1|\n",
      "|136667726|1798010,1798020|My childhood! \\nA...|    1|\n",
      "|136667270|1798010,1798020|Played before on ...|    1|\n",
      "|136667060|1798010,1798020|A classic series ...|    1|\n",
      "|136760666|        1798010|I remember playin...|    1|\n",
      "|136758902|        2273470|A new simulator. ...|    1|\n",
      "|136758985|        2273470|A small and fun d...|    1|\n",
      "+---------+---------------+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/18 19:33:12 WARN DAGScheduler: Broadcasting large task binary with size 6.0 MiB\n",
      "23/05/18 19:36:24 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(793, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Check if the dataframe is populated correctly\n",
    "df.select(df.columns[:20]).show()\n",
    "print((df.count(), len(df.columns)))\n",
    "# NOTE: shape() is not valid for pyspark dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/18 19:36:52 WARN DAGScheduler: Broadcasting large task binary with size 6.0 MiB\n",
      "23/05/18 19:40:05 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "23/05/18 19:40:07 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|label|count|\n",
      "+-----+-----+\n",
      "|    0|  146|\n",
      "|    1|  647|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Count class distribution\n",
    "count_df = df.groupBy('label').count().orderBy('label')\n",
    "count_df.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing predictive model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and test set\n",
    "df, test= df.randomSplit(weights = [0.70, 0.30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess review data: tokenize\n",
    "step1 = RegexTokenizer(inputCol=\"review_text\", outputCol=\"tokens\", pattern=\"\\\\W\")\n",
    "\n",
    "# Preprocess review data: remove stopwords and add slang words generated by ChatGPT\n",
    "swr = StopWordsRemover(inputCol='tokens', outputCol='filtered_words')\n",
    "w_slang = swr.getStopWords() + ['af', 'aight', 'amirite', 'anyways', 'awesomesauce', 'bae', 'bruh', 'btw', 'dope', 'fam', 'fr', 'gg', 'gj', 'glhf', 'grats', 'imho', 'imo', 'lmao', 'lmfao', 'lol', 'nvm', 'omg', 'pls', 'rip', 'salty', 'savage', 'smh', 'tbh', 'thx', 'ttyl', 'ty', 'wp', 'wtf', 'yeet', 'yolo']\n",
    "step2 = StopWordsRemover(inputCol='tokens', outputCol='filtered_words', stopWords=w_slang)\n",
    "\n",
    "# Preprocess review data: vectorize\n",
    "step3 = CountVectorizer(inputCol=\"filtered_words\", outputCol=\"features\")\n",
    "\n",
    "# Initiate logistic regression model\n",
    "step4 = LogisticRegression(featuresCol='features', labelCol='label')\n",
    "\n",
    "# Create pipeline\n",
    "pipeline = Pipeline(stages=[step1, step2, step3, step4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Preprocess review data: tokenize\n",
    "# step1 = RegexTokenizer(inputCol=\"review_text\", outputCol=\"tokens\", pattern=\"\\\\W\")\n",
    "\n",
    "# # Preprocess review data: remove stopwords and add slang words generated by ChatGPT\n",
    "# swr = StopWordsRemover(inputCol='tokens', outputCol='filtered_words')\n",
    "# w_slang = swr.getStopWords() + ['af', 'aight', 'amirite', 'anyways', 'awesomesauce', 'bae', 'bruh', 'btw', 'dope', 'fam', 'fr', 'gg', 'gj', 'glhf', 'grats', 'imho', 'imo', 'lmao', 'lmfao', 'lol', 'nvm', 'omg', 'pls', 'rip', 'salty', 'savage', 'smh', 'tbh', 'thx', 'ttyl', 'ty', 'wp', 'wtf', 'yeet', 'yolo']\n",
    "# step2 = StopWordsRemover(inputCol='tokens', outputCol='filtered_words', stopWords=w_slang)\n",
    "\n",
    "# # Preprocess review data: compute term frequency\n",
    "# step3 = HashingTF(inputCol=\"filtered_words\", outputCol=\"term_freq\", numFeatures=2**16)\n",
    "\n",
    "# # Preprocess review data: compute TF-IDF features\n",
    "# step4 = IDF(inputCol=\"term_freq\", outputCol=\"features\")\n",
    "\n",
    "# # Initiate logistic regression model\n",
    "# step5 = LogisticRegression(featuresCol='features', labelCol='label')\n",
    "\n",
    "# # Create pipeline\n",
    "# pipeline = Pipeline(stages=[step1, step2, step3, step4, step5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/18 19:40:33 WARN DAGScheduler: Broadcasting large task binary with size 5.6 MiB\n",
      "23/05/18 19:43:31 WARN DAGScheduler: Broadcasting large task binary with size 2.9 MiB\n",
      "23/05/18 19:43:57 WARN DAGScheduler: Broadcasting large task binary with size 5.6 MiB\n",
      "23/05/18 19:47:18 WARN DAGScheduler: Broadcasting large task binary with size 5.6 MiB\n",
      "23/05/18 19:50:37 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "23/05/18 19:50:42 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "23/05/18 19:50:43 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "23/05/18 19:50:43 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "23/05/18 19:50:44 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "23/05/18 19:50:44 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "23/05/18 19:50:45 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "23/05/18 19:50:45 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "23/05/18 19:50:45 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "23/05/18 19:50:46 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "23/05/18 19:50:46 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "23/05/18 19:50:46 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "23/05/18 19:50:47 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "23/05/18 19:50:47 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "23/05/18 19:50:47 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "23/05/18 19:50:48 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "23/05/18 19:50:48 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "23/05/18 19:50:48 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "23/05/18 19:50:49 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "23/05/18 19:50:49 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "23/05/18 19:50:50 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "23/05/18 19:50:50 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "23/05/18 19:50:50 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "23/05/18 19:50:51 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "23/05/18 19:50:51 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "23/05/18 19:50:52 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "23/05/18 19:50:52 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "23/05/18 19:50:52 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "23/05/18 19:50:53 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "23/05/18 19:50:53 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "23/05/18 19:50:53 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "23/05/18 19:50:54 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "23/05/18 19:50:54 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "23/05/18 19:50:54 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "23/05/18 19:50:55 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "23/05/18 19:50:55 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "23/05/18 19:50:55 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "23/05/18 19:50:56 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "23/05/18 19:51:45 WARN DAGScheduler: Broadcasting large task binary with size 5.6 MiB\n",
      "23/05/18 20:12:13 WARN DAGScheduler: Broadcasting large task binary with size 5.6 MiB\n",
      "[Stage 101:==================================================>(1075 + 1) / 1076]\r"
     ]
    }
   ],
   "source": [
    "# Fit pipeline to training data\n",
    "pipelineFit = pipeline.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/18 20:42:24 WARN DAGScheduler: Broadcasting large task binary with size 6.0 MiB\n",
      "23/05/18 21:11:54 WARN DAGScheduler: Broadcasting large task binary with size 3.5 MiB\n",
      "23/05/18 21:12:32 WARN DAGScheduler: Broadcasting large task binary with size 6.0 MiB\n",
      "23/05/18 21:49:19 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8303571428571429"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Validate model on test data\n",
    "predictions = pipelineFit.transform(test)\n",
    "accuracy = predictions.filter(predictions.label == predictions.prediction).count() / float(test.count())\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Save trained model\n",
    "pipelineFit.save(\"/home/linas/Desktop/kul/$em 2/advanced analytics in business/assignment 3/model/\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use trained model to make predictions as the stream comes in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper thread to avoid the Spark StreamingContext from blocking Jupyter\n",
    "        \n",
    "class StreamingThread(threading.Thread):\n",
    "    def __init__(self, ssc):\n",
    "        super().__init__()\n",
    "        self.ssc = ssc\n",
    "    def run(self):\n",
    "        self.ssc.start()\n",
    "        self.ssc.awaitTermination()\n",
    "    def stop(self):\n",
    "        print('----- Stopping... this may take a few seconds -----')\n",
    "        self.ssc.stop(stopSparkContext=False, stopGraceFully=True)\n",
    "        \n",
    "\n",
    "# Add capability to predict as the stream comes in\n",
    "globals()['models_loaded'] = False\n",
    "globals()['my_model'] = None\n",
    "\n",
    "def process(time, rdd):\n",
    "    if rdd.isEmpty():\n",
    "        return\n",
    "    \n",
    "    print(\"========= %s =========\" % str(time))\n",
    "    \n",
    "    # Convert to data frame\n",
    "    df = spark.read.json(rdd)\n",
    "    df.show()\n",
    "    # Load in the model if not yet loaded:\n",
    "    if not globals()['models_loaded']:\n",
    "        # load in your models here\n",
    "        globals()['my_model'] = PipelineModel.load('/home/linas/Desktop/kul/$em 2/advanced analytics in business/assignment 3/model/')\n",
    "        globals()['models_loaded'] = True\n",
    "    # Make predictions with loaded model\n",
    "    df_result = globals()['my_model'].transform(df)\n",
    "    df_result.select(\"app_id\", \"label\", \"review_id\", \"review_text\", \"prediction\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/linas/anaconda3/envs/pyspark/lib/python3.11/site-packages/pyspark/streaming/context.py:72: FutureWarning: DStream is deprecated as of Spark 3.4.0. Migrate to Structured Streaming.\n",
      "  warnings.warn(\n",
      "23/05/19 13:11:18 WARN StreamingContext: spark.master should be set as local[n], n > 1 in local mode if you have receivers to get data, otherwise Spark jobs will not get resources to process the received data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/19 13:11:20 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/05/19 13:11:20 WARN BlockManager: Block input-0-1684494680400 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/05/19 13:11:23 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/05/19 13:11:23 WARN BlockManager: Block input-0-1684494683400 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/05/19 13:11:24 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/05/19 13:11:24 WARN BlockManager: Block input-0-1684494684600 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/05/19 13:11:25 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/05/19 13:11:25 WARN BlockManager: Block input-0-1684494685600 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/05/19 13:11:26 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/05/19 13:11:26 WARN BlockManager: Block input-0-1684494686600 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/05/19 13:11:28 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/05/19 13:11:28 WARN BlockManager: Block input-0-1684494688600 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/05/19 13:11:30 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/05/19 13:11:30 WARN BlockManager: Block input-0-1684494690600 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/05/19 13:11:31 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/05/19 13:11:31 WARN BlockManager: Block input-0-1684494691400 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/05/19 13:11:34 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/05/19 13:11:34 WARN BlockManager: Block input-0-1684494694400 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/05/19 13:11:35 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/05/19 13:11:35 WARN BlockManager: Block input-0-1684494695400 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/05/19 13:11:56 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/05/19 13:11:56 WARN BlockManager: Block input-0-1684494716400 replicated to only 0 peer(s) instead of 1 peers\n"
     ]
    }
   ],
   "source": [
    "# Start streaming and predicting\n",
    "ssc = StreamingContext(sc, 10)\n",
    "lines = ssc.socketTextStream(\"seppe.net\", 7778)\n",
    "lines.foreachRDD(process)\n",
    "ssc_t = StreamingThread(ssc)\n",
    "ssc_t.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Stopping... this may take a few seconds -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/19 13:11:59 WARN SocketReceiver: Error receiving data\n",
      "java.net.SocketException: Socket closed\n",
      "\tat java.net.SocketInputStream.socketRead0(Native Method)\n",
      "\tat java.net.SocketInputStream.socketRead(SocketInputStream.java:116)\n",
      "\tat java.net.SocketInputStream.read(SocketInputStream.java:171)\n",
      "\tat java.net.SocketInputStream.read(SocketInputStream.java:141)\n",
      "\tat sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:284)\n",
      "\tat sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:326)\n",
      "\tat sun.nio.cs.StreamDecoder.read(StreamDecoder.java:178)\n",
      "\tat java.io.InputStreamReader.read(InputStreamReader.java:184)\n",
      "\tat java.io.BufferedReader.fill(BufferedReader.java:161)\n",
      "\tat java.io.BufferedReader.readLine(BufferedReader.java:324)\n",
      "\tat java.io.BufferedReader.readLine(BufferedReader.java:389)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.getNext(SocketInputDStream.scala:121)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.getNext(SocketInputDStream.scala:119)\n",
      "\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.receive(SocketInputDStream.scala:91)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$1.run(SocketInputDStream.scala:72)\n",
      "23/05/19 13:11:59 ERROR ReceiverTracker: Deregistered receiver for stream 0: Stopped by driver\n",
      "23/05/19 13:11:59 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Error receiving data\n",
      "java.net.SocketException: Socket closed\n",
      "\tat java.net.SocketInputStream.socketRead0(Native Method)\n",
      "\tat java.net.SocketInputStream.socketRead(SocketInputStream.java:116)\n",
      "\tat java.net.SocketInputStream.read(SocketInputStream.java:171)\n",
      "\tat java.net.SocketInputStream.read(SocketInputStream.java:141)\n",
      "\tat sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:284)\n",
      "\tat sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:326)\n",
      "\tat sun.nio.cs.StreamDecoder.read(StreamDecoder.java:178)\n",
      "\tat java.io.InputStreamReader.read(InputStreamReader.java:184)\n",
      "\tat java.io.BufferedReader.fill(BufferedReader.java:161)\n",
      "\tat java.io.BufferedReader.readLine(BufferedReader.java:324)\n",
      "\tat java.io.BufferedReader.readLine(BufferedReader.java:389)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.getNext(SocketInputDStream.scala:121)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.getNext(SocketInputDStream.scala:119)\n",
      "\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.receive(SocketInputDStream.scala:91)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$1.run(SocketInputDStream.scala:72)\n",
      "23/05/19 13:11:59 WARN ReceiverSupervisorImpl: Receiver has been stopped\n",
      "Exception in thread \"receiver-supervisor-future-0\" java.lang.Error: java.lang.InterruptedException: sleep interrupted\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1155)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: java.lang.InterruptedException: sleep interrupted\n",
      "\tat java.lang.Thread.sleep(Native Method)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:196)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\t... 2 more\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========= 2023-05-19 13:11:30 =========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+---------+--------------------+\n",
      "| app_id|label|review_id|         review_text|\n",
      "+-------+-----+---------+--------------------+\n",
      "| 669330|    1|138534240|Loads of cool des...|\n",
      "| 669330|    1|138533323|Very fun it has a...|\n",
      "|2369390|    0|138534914|Extraordinarily b...|\n",
      "|2369390|    1|138534604|Vote me as new El...|\n",
      "|2369390|    1|138534371|You're the Lucky ...|\n",
      "|2369390|    1|138534307|fun game with all...|\n",
      "+-------+-----+---------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+---------+--------------------+----------+\n",
      "| app_id|label|review_id|         review_text|prediction|\n",
      "+-------+-----+---------+--------------------+----------+\n",
      "| 669330|    1|138534240|Loads of cool des...|       1.0|\n",
      "| 669330|    1|138533323|Very fun it has a...|       0.0|\n",
      "|2369390|    0|138534914|Extraordinarily b...|       0.0|\n",
      "|2369390|    1|138534604|Vote me as new El...|       1.0|\n",
      "|2369390|    1|138534371|You're the Lucky ...|       1.0|\n",
      "|2369390|    1|138534307|fun game with all...|       1.0|\n",
      "+-------+-----+---------+--------------------+----------+\n",
      "\n",
      "========= 2023-05-19 13:11:40 =========\n",
      "+-------+-----+---------+--------------------+\n",
      "| app_id|label|review_id|         review_text|\n",
      "+-------+-----+---------+--------------------+\n",
      "|2369390|    1|138533419|no comments on ga...|\n",
      "|2096620|    0|138533266|at first glace, i...|\n",
      "|2381160|    1|138534749|Cute little first...|\n",
      "|2382570|    0|138535368|With the game bei...|\n",
      "+-------+-----+---------+--------------------+\n",
      "\n",
      "+-------+-----+---------+--------------------+----------+\n",
      "| app_id|label|review_id|         review_text|prediction|\n",
      "+-------+-----+---------+--------------------+----------+\n",
      "|2369390|    1|138533419|no comments on ga...|       0.0|\n",
      "|2096620|    0|138533266|at first glace, i...|       0.0|\n",
      "|2381160|    1|138534749|Cute little first...|       0.0|\n",
      "|2382570|    0|138535368|With the game bei...|       0.0|\n",
      "+-------+-----+---------+--------------------+----------+\n",
      "\n",
      "========= 2023-05-19 13:12:00 =========\n",
      "+-------+-----+---------+--------------------+\n",
      "| app_id|label|review_id|         review_text|\n",
      "+-------+-----+---------+--------------------+\n",
      "|1983710|    1|138533335|This game sucked ...|\n",
      "+-------+-----+---------+--------------------+\n",
      "\n",
      "+-------+-----+---------+--------------------+----------+\n",
      "| app_id|label|review_id|         review_text|prediction|\n",
      "+-------+-----+---------+--------------------+----------+\n",
      "|1983710|    1|138533335|This game sucked ...|       1.0|\n",
      "+-------+-----+---------+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ssc_t.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
